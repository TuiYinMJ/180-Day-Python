# 20、初探网络爬虫

## 1、网络基础

​	爬虫又叫网络蜘蛛，按照一定的规则，自动抓取网页信息

​	协议是数据传输和数据解释的规则，也就是如何发送，如何解开阅读，就像两个人对话，我说中文，你说英文，各说各的，谁也听不懂谁的

![img](https://raw.githubusercontent.com/TuiYinMJ/Python-img/master/Python-img/202307221313109.jpeg)



## 2、网络请求

![image-20230722132110101](https://raw.githubusercontent.com/TuiYinMJ/Python-img/master/Python-img/202307221321130.png)

​	我们利用F12网页开发者工具抓包，一般请求头如上组成，百度的比较多

- GET方式访问，访问的index.htm页面，使用的HTTP协议1.1版本
- Host，被请求资源的主机头
- Connection，keep-alive持久连接
- Cache-Control: no-cache，缓存
- User-Agent，请求浏览器和系统信息
- Accept，客户端希望接收哪些类型的信息
- Accept-Encoding，可以接收的编码格式
- Accept-Language，可接收的语言
- Cookie，大多数用于验证

​	除了一一行，其他内容可以归类为消息报头

![image-20230722132615651](https://raw.githubusercontent.com/TuiYinMJ/Python-img/master/Python-img/202307221326685.png)

​	上图响应头，200 OK表示响应成功

- 1xx 消息，已经被接收，继续处理
- 2xx 成功，请求已被服务器接收、理解且接受
- 3xx 重定向，请求被定向到了其他地址
- 4xx 错误，请求错误或者无法被执行
- 5xx 服务器错误，服务器在处理某个正确请求的时候发生错误



- server代表服务器用来处理的软件信息
- Content-Type，响应类型
- Content-Length，回应数据长度，这次没有



## 3、三次握手与四次挥手

![img](https://raw.githubusercontent.com/TuiYinMJ/Python-img/master/Python-img/202307221332050.png)

![img](https://raw.githubusercontent.com/TuiYinMJ/Python-img/master/Python-img/202307221333830.png)



## 4、requests模块

```python
# coding : UTF-8

import requests

if __name__ == '__main__':
    response = requests.get(url='https://www.baidu.com/')
    print(response.text)
```

​	上面是一个简单的使用方法，使用了一个get请求，获取了返回内容text

​	params参数可以自定义构造链接

```python
# coding : UTF-8

import requests

if __name__ == '__main__':
    data = {'phone':'13800138000', 'code': '123'}
    response = requests.post(url='http://httpbin.org/post',data=data)
    print(response.text)
```

​	一个post请求的使用方法，data是提交的数据，是必须的



- .url，查看返回数据的url
- .headers，响应头
- .text，返回体
- .content，二进制方式返回
- .json，返回json格式
- .status_code，状态码
- .request.headers，请求头
- .cookies，返回cookies

```python
# coding : UTF-8

import requests

if __name__ == '__main__':
    header = {
        'user-agent': '',
        'Accept': '',
        'cookie':''
    }

    response = requests.get(url='http://httpbin.org/get',headers=header,timeout=1)
    print(response.headers)
```

​	构造自定义请求头，timeout超时，1秒钟内没返回就终止链接



### 1、cookie操作

```python
# coding : UTF-8

import requests

if __name__ == '__main__':
    cookie = dict(cookie='hello world')
    response = requests.get(url='http://httpbin.org/cookies',cookies=cookie)
    print(response.text)
```

​	字典构造cookie



### 2、代理设置

​	一个ip访问过多会封ip，所以有时候解决封禁ip要使用代理隐藏真实ip

​	利用proxies参数

```python
proxy = {
    "http:":"http://用户名:密码@代理接口信息",
    "https:":"https://用户名:密码@代理接口信息"
}
```



### 3、证书跳过

​	SSL的verify参数默认开启，也就是会检查一些SSL，若验证失败会异常

​	若将verify设置为False，则不会出现这些情况

​	也可以将verify='证书路径'来指定



### 4、小案例

```python
# coding : UTF-8

import requests

if __name__ == '__main__':
    for i in range(1, 5):
        url = 'http://yushu.talelin.com/book/search?q=python&page={}'.format(i)
        response = requests.get(url=url,)
        with  open('{}.html'.format(i), 'w',encoding='utf-8') as f:
            f.write(response.text)
```

​	当然了，这只是一个小案例，并不完美，在这里可以利用正则，对返回内容取出关键处，然后组合写出到文件，自己试试吧



## 5、正则应用

​	正则可以匹配文本的片段，就比如我们上面小案例，利用正则匹配出书名、作者、价格、书籍介绍等等

- match，头部匹配
- search，查找任何位置，只有一次
- findall，查找整个字符串，查找所有符合的

```python
# coding : UTF-8

import requests,re

if __name__ == '__main__':
    for i in range(1, 5):
        url = 'http://yushu.talelin.com/book/search?q=python&page={}'.format(i)
        response = requests.get(url=url)
        pattern = r'title">(.*?)</span>\s*<span>(.*?)</span>\s*<span class="summary">(.*?)</span>'
        ret = re.findall(pattern,response.text)
        for i in range(0,len(ret)):
            print(ret[i])
            with open('python.json','a+',encoding='UTF-8') as f:
                f.write(f'书名:{str(ret[i][0])}\n作者价格:{str(ret[i][1])}\n书本介绍: {str(ret[i][2])}\n\n')
```

​	简陋版本，自己优化，这是一个很不好的方式



## 6、xpath

​	正则容易出错，使用xpath提取节点再根据方法提取内容，更简单更不容易出错

​	了解一些xpath的路径表达式

- /div 从根节点开始选择div节点
- /span 同上
- //a 选取文档里的所有a节点，不考虑位置
- @class 选取名为class的属性
- . 选取当前节点
- .. 选取当前父节点
- //* 选取所有元素
- //@* 选取所有带属性的元素

​	比如：//span[@class='inq']/text()

​	获取所有的span，并且class=inq的，获取text()纯文本内容

​	其他的自己百度看吧，太简单了



## 7、BeautifulSoup

​	更简便的解析网页里的数据，依靠解析器解析数据

- BeautifulSoup(markup,"html.parser")
- BeautifulSoup(markup,"lxml")

​	第一个参数是需要解析的html内容，第二个是解析器，基本用lxml即可，第一个也可以

```python
soup = BeautifulSoup(rep.text, 'lxml')  # 网页内容解析成BeautifulSoup对象
movies = soup.find_all('div', class_='info')  # 把所有class=info的div代码块找出，也就是信息块
```

